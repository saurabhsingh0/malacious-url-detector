{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4864816",
   "metadata": {},
   "source": [
    "## Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8b4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from joblib import dump, load\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339559d",
   "metadata": {},
   "source": [
    "## Creating all the possible combinations of n-grams using lowercase alphabets and digits and storing in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72c3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.inf)\n",
    "alphanum = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','0','1','2','3','4','5','6','7','8','9']\n",
    "permutations = itertools.product(alphanum, repeat=3)\n",
    "featuresDict = {}\n",
    "counter = 0\n",
    "for perm in permutations:\n",
    "    #print(perm)\n",
    "    f=''\n",
    "    for char in perm:\n",
    "        f = f+char;\n",
    "    featuresDict[(''.join(perm))] = counter\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f415361",
   "metadata": {},
   "source": [
    "## A function that takes a sentence as imput and returns list of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66387180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram(sentence):\n",
    "    s = sentence.lower()\n",
    "    s = ''.join(e for e in s if e.isalnum()) #replace spaces and slashes\n",
    "    processedList = []\n",
    "    for tup in list(ngrams(s,3)):\n",
    "        processedList.append((''.join(tup)))\n",
    "    return processedList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4a449",
   "metadata": {},
   "source": [
    "## A function that accepts a dataframe and 2 arrays X & y. X denotes the features that will be used for training and y is the label \n",
    "## URL is then stripped down to the suitable format for pre-processing like removing unwanted Dots, Slash , WWW. , .com etc.\n",
    "## This data is then transferred to the pre-processing module for conversion into suitable format for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66869978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(dataframe, X, y):\n",
    "    #print(dataframe)\n",
    "    for index,row in df.iterrows():\n",
    "        url = row['url'].strip().replace(\"https://\",\"\")\n",
    "        url = row['url'].strip().replace(\"http://\",\"\")\n",
    "        url = url.replace(\"http://\",\"\")\n",
    "        url = re.sub(r'\\.[A-Za-z0-9]+/*','',url)\n",
    "        for gram in generate_ngram(url):\n",
    "            try:\n",
    "                X[index][featuresDict[gram]] = X[index][featuresDict[gram]] + 1\n",
    "            except:\n",
    "                print(gram,\"doesn't exist\")\n",
    "        y[index] = int(row['label'])\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7f999",
   "metadata": {},
   "source": [
    "## Load the dataset and split it into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523b1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataframe = pd.read_csv(\"malicious-url-detector_dataset.csv\")\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452ecf0",
   "metadata": {},
   "source": [
    "## The dataset has 1 million rows. \n",
    "## we will be using SGDClassifier which by default fits a linear support vector machine (SVM).\n",
    "## This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3d5602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiه doesn't exist\n",
      "iه½ doesn't exist\n",
      "ه½è doesn't exist\n",
      "½è¹ doesn't exist\n",
      "è¹ه doesn't exist\n",
      "¹هن doesn't exist\n",
      "هن½ doesn't exist\n",
      "ن½ه doesn't exist\n",
      "½ه½ doesn't exist\n",
      "ه½è doesn't exist\n",
      "½è¹ doesn't exist\n",
      "è¹ه doesn't exist\n",
      "¹هڈ doesn't exist\n",
      "هڈو doesn't exist\n",
      "ڈوه doesn't exist\n",
      "وهç doesn't exist\n",
      "هç² doesn't exist\n",
      "ç²ن doesn't exist\n",
      "²نه doesn't exist\n",
      "نهé doesn't exist\n",
      "هéن doesn't exist\n",
      "éن½ doesn't exist\n",
      "18æ doesn't exist\n",
      "8æ³ doesn't exist\n",
      "æ³å doesn't exist\n",
      "³åœ doesn't exist\n",
      "åœæ doesn't exist\n",
      "œæœ doesn't exist\n",
      "æœº doesn't exist\n",
      "œº3 doesn't exist\n",
      "º32 doesn't exist\n",
      "e4ä doesn't exist\n",
      "4ää doesn't exist\n",
      "ääš doesn't exist\n",
      "äšç doesn't exist\n",
      "šçè doesn't exist\n",
      "çèç doesn't exist\n",
      "èçˆ doesn't exist\n",
      "çˆï doesn't exist\n",
      "ˆï¼ doesn't exist\n",
      "ï¼ˆ doesn't exist\n",
      "¼ˆå doesn't exist\n",
      "ˆåå doesn't exist\n",
      "ååº doesn't exist\n",
      "åºå doesn't exist\n",
      "ºåˆ doesn't exist\n",
      "åˆå doesn't exist\n",
      "ˆåï doesn't exist\n",
      "åï¼ doesn't exist\n",
      "ï¼3 doesn't exist\n",
      "¼32 doesn't exist\n",
      "wnا doesn't exist\n",
      "nاه doesn't exist\n",
      "اهء doesn't exist\n",
      "هءم doesn't exist\n",
      "aiه doesn't exist\n",
      "iه½ doesn't exist\n",
      "ه½è doesn't exist\n",
      "½è¹ doesn't exist\n",
      "è¹ه doesn't exist\n",
      "¹هن doesn't exist\n",
      "هن½ doesn't exist\n",
      "ن½ه doesn't exist\n",
      "½ه½ doesn't exist\n",
      "ه½è doesn't exist\n",
      "½è¹ doesn't exist\n",
      "è¹ه doesn't exist\n",
      "¹هڈ doesn't exist\n",
      "هڈو doesn't exist\n",
      "ڈوه doesn't exist\n",
      "وهç doesn't exist\n",
      "هç² doesn't exist\n",
      "ç²ن doesn't exist\n",
      "²نه doesn't exist\n",
      "نهé doesn't exist\n",
      "هéن doesn't exist\n",
      "éن½ doesn't exist\n",
      "wnا doesn't exist\n",
      "nاه doesn't exist\n",
      "اهء doesn't exist\n",
      "هءم doesn't exist\n",
      "wnو doesn't exist\n",
      "nو² doesn't exist\n",
      "و²³ doesn't exist\n",
      "²³2 doesn't exist\n",
      "³2b doesn't exist\n",
      "reä doesn't exist\n",
      "eää doesn't exist\n",
      "ääš doesn't exist\n",
      "äšç doesn't exist\n",
      "šçˆ doesn't exist\n",
      "çˆå doesn't exist\n",
      "ˆåœ doesn't exist\n",
      "åœç doesn't exist\n",
      "œç¾ doesn't exist\n",
      "ç¾ž doesn't exist\n",
      "¾žç doesn't exist\n",
      "žçè doesn't exist\n",
      "çèç doesn't exist\n",
      "èçˆ doesn't exist\n",
      "çˆ3 doesn't exist\n",
      "ˆ32 doesn't exist\n",
      "16ه doesn't exist\n",
      "6هé doesn't exist\n",
      "هéœ doesn't exist\n",
      "éœ² doesn't exist\n",
      "œ²ه doesn't exist\n",
      "²هه doesn't exist\n",
      "ههه doesn't exist\n",
      "ههچ doesn't exist\n",
      "هچˆ doesn't exist\n",
      "چˆه doesn't exist\n",
      "ˆهœ doesn't exist\n",
      "هœ6 doesn't exist\n",
      "œ60 doesn't exist\n",
      "azä doesn't exist\n",
      "zä½ doesn't exist\n",
      "ä½³ doesn't exist\n",
      "½³è doesn't exist\n",
      "³èƒ doesn't exist\n",
      "èƒ½ doesn't exist\n",
      "ƒ½ç doesn't exist\n",
      "½çå doesn't exist\n",
      "çåš doesn't exist\n",
      "åšæ doesn't exist\n",
      "šæå doesn't exist\n",
      "æåè doesn't exist\n",
      "åè½ doesn't exist\n",
      "è½ä doesn't exist\n",
      "½äc doesn't exist\n",
      "äca doesn't exist\n",
      "wnو doesn't exist\n",
      "nو² doesn't exist\n",
      "و²³ doesn't exist\n",
      "²³2 doesn't exist\n",
      "³2b doesn't exist\n",
      "16ه doesn't exist\n",
      "6هé doesn't exist\n",
      "هéœ doesn't exist\n",
      "éœ² doesn't exist\n",
      "œ²ه doesn't exist\n",
      "²هه doesn't exist\n",
      "ههه doesn't exist\n",
      "ههچ doesn't exist\n",
      "هچˆ doesn't exist\n",
      "چˆه doesn't exist\n",
      "ˆهœ doesn't exist\n",
      "هœ6 doesn't exist\n",
      "œ60 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "no_of_rows = 25000\n",
    "no_of_batches = int(train_df.shape[0]/no_of_rows) +1\n",
    "classifier = SGDClassifier()\n",
    "#print(no_of_batches)\n",
    "for i in range(0, no_of_batches):\n",
    "    start = no_of_rows*i\n",
    "    if start + no_of_rows > train_df.shape[0] :\n",
    "        df = train_df.iloc[start:,:]\n",
    "    else :\n",
    "        df = train_df.iloc[start:start+no_of_rows, :]\n",
    "    df = df.reset_index()\n",
    "    (X,y) = preprocess_sentences(df, \\\n",
    "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
    "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
    "    classifier.partial_fit(X, y, classes=np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc58f50",
   "metadata": {},
   "source": [
    "## Testing the model against the test set and getting the count of correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33688539",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rows = 25000\n",
    "no_of_batches = int(test_df.shape[0]/no_of_rows) +1\n",
    "#print(no_of_batches)\n",
    "correct = 0;\n",
    "incorrect = 0\n",
    "for i in range(0, no_of_batches):\n",
    "    start = no_of_rows*i\n",
    "    if start + no_of_rows > train_df.shape[0] :\n",
    "        df = train_df.iloc[start:,:]\n",
    "    else :\n",
    "        df = test_df.iloc[start:start+no_of_rows, :]\n",
    "    df = df.reset_index()\n",
    "    (X_test,y_test) = preprocess_sentences(df, \\\n",
    "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
    "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    for index,row in df.iterrows():\n",
    "        if row['label'] == y_pred[index]:\n",
    "            correct = correct+1\n",
    "        else:\n",
    "            incorrect = incorrect + 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d148c",
   "metadata": {},
   "source": [
    "## Displaying the result of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct Predictions \", correct)\n",
    "print(\"Incorrect Predictions \", incorrect)\n",
    "accuracy = (correct/test_df.shape[0])*100\n",
    "print(\"Accuracy of the model is: \"'{0:.4g}'.format(accuracy), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803c3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences_url(url):\n",
    "    X= np.zeros([1, 46656],dtype=\"int\")\n",
    "    url = url.strip().replace(\"https://\",\"\")\n",
    "    url = url.replace(\"http://\",\"\")\n",
    "    url = re.sub(r'\\.[A-Za-z0-9]+/*','',url)\n",
    "    for gram in generate_ngram(url):\n",
    "        try:\n",
    "            X[0][featuresDict[gram]] = X[0][featuresDict[gram]] + 1\n",
    "            #print('preprocess_sentences')\n",
    "            #print(X[index][featuresDict[gram]])\n",
    "        except:\n",
    "            print(gram,\"doesn't exist\")\n",
    "    return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582e61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.itidea.it/centroesteticosothys/img/_notes/gum.exe  is malacious!\n"
     ]
    }
   ],
   "source": [
    "url = \"www.itidea.it/centroesteticosothys/img/_notes/gum.exe\"\n",
    "test = preprocess_sentences_url(url)\n",
    "pred = classifier.predict(test)\n",
    "if pred == 1 :\n",
    "    print(url, \" is malacious!\")\n",
    "else:\n",
    "    print(url, \" is not malacious!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
